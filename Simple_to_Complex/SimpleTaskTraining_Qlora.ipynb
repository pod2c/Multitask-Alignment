{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f3a80c-6ae0-4e11-82f8-ee96b42b5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ec8202-24a5-490e-86e3-0e586da54fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# åŠ è½½sciqæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26dbe92-0dd4-4d18-88f1-fafbc78bd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "train_data = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6a60ef-c15b-4361-807c-53aac4d20d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»åŸå§‹æ•°æ®é›†ä¸­éšæœºæŠ½å–10000æ¡æ•°æ®\n",
    "train_data = train_data.shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b7ea57-2c4b-4289-a286-05d1b768ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†txtå’Œlabelå­—æ®µåˆå¹¶ä¸ºä¸€ä¸ªæ•´ä½“\n",
    "def merge_fields(example):\n",
    "    text = example['text']\n",
    "    label = example['label']\n",
    "    merged_input = f\"Text: {text}\\nLabel: {label}\"\n",
    "    return {'merged_input': merged_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8c04ea-72ff-45bd-8346-a6ec715d810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a7574921e74866b1e6ef61f8d43598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(merge_fields, remove_columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7902b29d-d506-4b06-894c-1b573a4b017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6c355dbdc94fd0bef79e968c62d942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = 'llama3'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fbc0173-a009-4096-86af-c3985b90e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0fbe7f-ebdf-4e26-ac47-e1043b3f16bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762f8562-3b47-4fc4-933e-54207a12439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = './results'\n",
    "per_device_train_batch_size = 64\n",
    "gradient_accumulation_steps = 64\n",
    "optim = 'paged_adamw_32bit'\n",
    "save_steps = 1000\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4  # Typically higher for LoRA\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = 'constant'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    fp16=True,  # Mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e707d1fc-7e82-4925-a56c-cc9d4dc84fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605d434fca4b4c908c810ea394a70af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8])\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 512\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    dataset_text_field='merged_input',\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f781cbe-2959-4bc0-a2a3-6e43b48f74ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms1820587\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240624_124520-7y5ef1di</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s1820587/huggingface/runs/7y5ef1di' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/s1820587/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s1820587/huggingface' target=\"_blank\">https://wandb.ai/s1820587/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s1820587/huggingface/runs/7y5ef1di' target=\"_blank\">https://wandb.ai/s1820587/huggingface/runs/7y5ef1di</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/6 08:34 < 25:44, 0.00 it/s, Epoch 0.82/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f693-a956-466f-91aa-9d29d39c337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA model\n",
    "trainer.model.save_pretrained(\"finetuned_lora_model\")\n",
    "\n",
    "# Merge the LoRA weights with the base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"finetuned_merged_model\")\n",
    "tokenizer.save_pretrained(\"finetuned_merged_model\")\n",
    "print(\"Trained model and tokenizer saved to finetuned_merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89b917-f0eb-4d09-9f9d-c2fdbec2e002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
