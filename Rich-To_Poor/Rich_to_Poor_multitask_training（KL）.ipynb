{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f2b787-d1a5-4d44-872c-7a679ac635e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f0e39c-0c71-4832-929a-3f64f6382700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms1820587\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240710_101531-6wn0u4bc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc' target=\"_blank\">squad-hh-rlhf-contrastive</a></strong> to <a href='https://wandb.ai/s1820587/multitask-lora-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s1820587/multitask-lora-finetuning' target=\"_blank\">https://wandb.ai/s1820587/multitask-lora-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc' target=\"_blank\">https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f81fa814370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化 wandb\n",
    "wandb.init(project=\"multitask-lora-finetuning\", name=\"squad-hh-rlhf-contrastive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe6a5bd-de26-4698-8b7f-8f531de97ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理函数\n",
    "def prepare_squad_dataset(example):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"No answer available.\"\n",
    "    \n",
    "    input_text = f\"Context: {context} Question: {question} Answer:\"\n",
    "    output_text = answer\n",
    "    \n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"output\": output_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c53cc8-352e-4f46-a244-5767da60af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hh_rlhf_dataset(example):\n",
    "    chosen_parts = example['chosen'].split('\\n\\nHuman: ')\n",
    "    rejected_parts = example['rejected'].split('\\n\\nHuman: ')\n",
    "    \n",
    "    if len(chosen_parts) > 1 and len(rejected_parts) > 1:\n",
    "        human_input = chosen_parts[1].split('\\n\\nAssistant: ')[0]\n",
    "        chosen_output = chosen_parts[1].split('\\n\\nAssistant: ')[1].split('\\n\\nHuman: ')[0]\n",
    "        rejected_output = rejected_parts[1].split('\\n\\nAssistant: ')[1].split('\\n\\nHuman: ')[0]\n",
    "        \n",
    "        return {\n",
    "            \"input\": human_input.strip(),\n",
    "            \"chosen_output\": chosen_output.strip(),\n",
    "            \"rejected_output\": rejected_output.strip()\n",
    "        }\n",
    "    else:\n",
    "        return {\"input\": \"\", \"chosen_output\": \"\", \"rejected_output\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68efc47a-5b6b-4c1b-a902-a7848e9877aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和处理数据集\n",
    "squad_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "hh_rlhf_dataset = load_dataset(\"hh-rlhf\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2e70de-932a-49ec-943c-418e716c914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_squad = squad_dataset.map(prepare_squad_dataset, remove_columns=squad_dataset.column_names)\n",
    "processed_hh_rlhf = hh_rlhf_dataset.map(prepare_hh_rlhf_dataset, remove_columns=hh_rlhf_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e875d4c-fbfa-4f24-9480-f4c8590faf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱数据集\n",
    "shuffled_squad = processed_squad.shuffle(seed=42)\n",
    "shuffled_rlhf = processed_hh_rlhf.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45b7a3d-c6a5-4b89-816b-4ff50fe40740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择子集（可选）\n",
    "processed_squad = shuffled_squad.select(range(5000))\n",
    "processed_hh_rlhf = shuffled_rlhf.select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9797102-55b3-4fce-b03f-89e747223b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'chosen_output', 'rejected_output'],\n",
       "     num_rows: 5000\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_squad, processed_hh_rlhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c200659d-126e-4d73-9d94-ad1da8fc12b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Context: The Roman Catholic Church canon law also includes the main five rites (groups) of churches which are in full union with the Roman Catholic Church and the Supreme Pontiff: Question: What term characterizes the intersection of the rites with the Roman Catholic Church? Answer:',\n",
       " 'output': 'full union'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_squad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d2ff443-92cc-45f2-8e8b-d25df63564f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Why did cells originally combine together to create life?',\n",
       " 'chosen_output': 'Because their simple components -- chemicals -- interacted in particular ways.  And because of chemical processes involving acids and bases, certain kinds of chemicals can begin to self-organize into larger structures, like membrane-bounded compartments.  And it’s from those compartments that life eventually emerged.',\n",
       " 'rejected_output': 'Cells combine because they benefit from cooperation, since they can have less competition for resources by working together.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_hh_rlhf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f547d5a-eeff-49ef-8ffe-399291fe66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型和tokenizer设置\n",
    "base_model_name = \"llama3\"  # 基础模型名称\n",
    "peft_model_path = \"finetuned_causal_model\"  # 替换为您保存的 QLoRA 权重路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f589a83a-8aa8-4913-91ac-cf0787f5164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63cd6be-5353-45cb-9d6c-706669e5b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化配置\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1278a0-c820-440b-95d1-00ba25b42f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4775cf4d91344f7a002ca9ea56e2f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载量化后的基础模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce5d0783-d1a8-4194-ac65-409ef2df9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载之前微调的 QLoRA 权重\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# 为进一步的 k-bit 训练准备模型\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8cafc6-d443-4eb5-807d-1001510b5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新的 LoRA 配置用于多任务训练\n",
    "new_peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a37101f-f374-4e06-85a9-c5358d21a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用新的 LoRA 配置\n",
    "model = get_peft_model(model, new_peft_config)\n",
    "\n",
    "# 记录模型配置到 wandb\n",
    "wandb.config.update({\n",
    "    \"base_model_name\": base_model_name,\n",
    "    \"peft_model_path\": peft_model_path,\n",
    "    \"new_lora_r\": new_peft_config.r,\n",
    "    \"new_lora_alpha\": new_peft_config.lora_alpha,\n",
    "    \"new_lora_dropout\": new_peft_config.lora_dropout,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc6a8b6-c77c-4e84-8edf-d44c60bb124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squad_loss(outputs, labels):\n",
    "    if isinstance(outputs, dict):\n",
    "        if 'logits' in outputs:\n",
    "            logits = outputs['logits']\n",
    "        else:\n",
    "            raise ValueError(\"Outputs dictionary does not contain 'logits'\")\n",
    "    else:\n",
    "        logits = outputs.logits\n",
    "\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea11409-1dbc-4225-84ea-0302986431fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hh_rlhf_kl_loss(model, tokenizer, inputs, chosen_outputs, rejected_outputs, device, max_length=512, alpha=0.1):\n",
    "    # 编码输入\n",
    "    input_encodings = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = input_encodings.input_ids.to(device)\n",
    "    attention_mask = input_encodings.attention_mask.to(device)\n",
    "    \n",
    "    # 编码chosen和rejected输出\n",
    "    chosen_encodings = tokenizer(chosen_outputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    chosen_ids = chosen_encodings.input_ids.to(device)\n",
    "    \n",
    "    rejected_encodings = tokenizer(rejected_outputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    rejected_ids = rejected_encodings.input_ids.to(device)\n",
    "    \n",
    "    # 确保所有张量的第一维度相同\n",
    "    min_length = min(input_ids.size(1), chosen_ids.size(1), rejected_ids.size(1))\n",
    "    input_ids = input_ids[:, :min_length]\n",
    "    attention_mask = attention_mask[:, :min_length]\n",
    "    chosen_ids = chosen_ids[:, :min_length]\n",
    "    rejected_ids = rejected_ids[:, :min_length]\n",
    "    \n",
    "    # 获取模型输出\n",
    "    with torch.no_grad():\n",
    "        reference_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    chosen_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=chosen_ids)\n",
    "    rejected_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=rejected_ids)\n",
    "    \n",
    "    # 计算对比损失\n",
    "    chosen_loss = chosen_outputs.loss\n",
    "    rejected_loss = rejected_outputs.loss\n",
    "    contrastive_loss = F.relu(chosen_loss - rejected_loss + 0.1)\n",
    "    \n",
    "    # 计算KL散度\n",
    "    kl_div = F.kl_div(\n",
    "        F.log_softmax(chosen_outputs.logits, dim=-1),\n",
    "        F.softmax(reference_outputs.logits, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # 组合损失\n",
    "    total_loss = contrastive_loss + alpha * kl_div\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38763269-5183-4854-8d15-f21960688bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input': [item['input'] for item in batch],\n",
    "        'output': [item.get('output', '') for item in batch],\n",
    "        'chosen_output': [item.get('chosen_output', '') for item in batch],\n",
    "        'rejected_output': [item.get('rejected_output', '') for item in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e48a1cb-9feb-4dae-ab0a-9285b3140c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataloader = DataLoader(processed_squad, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "hh_rlhf_dataloader = DataLoader(processed_hh_rlhf, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ad08b6-6ded-437f-9c0d-8c5f3807fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, squad_dataloader, hh_rlhf_dataloader, num_epochs, device, gradient_accumulation_steps):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    total_steps = min(len(squad_dataloader), len(hh_rlhf_dataloader)) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    model, optimizer, squad_dataloader, hh_rlhf_dataloader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, squad_dataloader, hh_rlhf_dataloader, scheduler\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training\", position=0, leave=True)\n",
    "\n",
    "    max_length = 512  # 设置最大长度\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for step, (squad_batch, hh_rlhf_batch) in enumerate(zip(squad_dataloader, hh_rlhf_dataloader)):\n",
    "            # SQuAD task\n",
    "            squad_inputs = tokenizer(squad_batch['input'], squad_batch['output'], \n",
    "                                     padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "            squad_inputs = {k: v.to(device) for k, v in squad_inputs.items()}\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                squad_outputs = model(**squad_inputs)\n",
    "                squad_loss = compute_squad_loss(squad_outputs, squad_inputs['input_ids'])\n",
    "                accelerator.backward(squad_loss)\n",
    "\n",
    "            # hh-rlhf task\n",
    "            hh_rlhf_loss = compute_hh_rlhf_kl_loss(\n",
    "                model,\n",
    "                tokenizer, \n",
    "                hh_rlhf_batch['input'], \n",
    "                hh_rlhf_batch['chosen_output'], \n",
    "                hh_rlhf_batch['rejected_output'], \n",
    "                device,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                accelerator.backward(hh_rlhf_loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += squad_loss.item() + hh_rlhf_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                'epoch': epoch+1, \n",
    "                'loss': total_loss / (step + 1),\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            }, refresh=True)\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"squad_loss\": squad_loss.item(),\n",
    "                \"hh_rlhf_kl_loss\": hh_rlhf_loss.item(),\n",
    "                \"total_loss\": squad_loss.item() + hh_rlhf_loss.item(),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "            }, step=epoch * len(squad_dataloader) + step)\n",
    "\n",
    "        avg_loss = total_loss / len(squad_dataloader)\n",
    "        progress_bar.set_postfix({'epoch': epoch+1, 'avg_loss': avg_loss}, refresh=True)\n",
    "        \n",
    "        # Log epoch average loss to wandb\n",
    "        wandb.log({\"epoch\": epoch, \"avg_loss\": avg_loss})\n",
    "\n",
    "    progress_bar.close()\n",
    "    return accelerator.unwrap_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a83812-d727-4d89-9e0b-32735b8c2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Training:   0%|          | 0/3750 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 3750/3750 [55:12<00:00,  1.13it/s, epoch=3, avg_loss=0.232]        \n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gradient_accumulation_steps = 4  # 设置梯度累积步数\n",
    "trained_model = train(model, tokenizer, squad_dataloader, hh_rlhf_dataloader, num_epochs=3, device=device, gradient_accumulation_steps=gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da96e748-3a92-41a9-bf83-43ba0f4afb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Model and tokenizer saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▁▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>hh_rlhf_kl_loss</td><td>▃▄▁▁▂▁▂▄▂▄▁▅▁▁█▂▂▂▂▄▁▂▂▁▁▂▁▁▅▃▁▃▁▂▁▁▁▂▁▁</td></tr><tr><td>learning_rate</td><td>▃████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>squad_loss</td><td>██▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_loss</td><td>██▄▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.23216</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>hh_rlhf_kl_loss</td><td>0.39842</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>squad_loss</td><td>0.05965</td></tr><tr><td>total_loss</td><td>0.45807</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">squad-hh-rlhf-contrastive</strong> at: <a href='https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc' target=\"_blank\">https://wandb.ai/s1820587/multitask-lora-finetuning/runs/6wn0u4bc</a><br/> View project at: <a href='https://wandb.ai/s1820587/multitask-lora-finetuning' target=\"_blank\">https://wandb.ai/s1820587/multitask-lora-finetuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 保存模型\n",
    "trained_model.save_pretrained(\"multitask_model1\")\n",
    "tokenizer.save_pretrained(\"multitask_model1\")\n",
    "\n",
    "print(\"Training completed. Model and tokenizer saved.\")\n",
    "\n",
    "# 结束 wandb 运行  \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00f8c5-e73f-4e48-9c19-5289eaed82a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
